{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5bf4b1f",
   "metadata": {},
   "source": [
    "# 프로젝트: 멋진 작사가 만들기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d3133d",
   "metadata": {},
   "source": [
    "# 1. 필요한 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c40316f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8567346a",
   "metadata": {},
   "source": [
    "# 2. 데이터 읽어오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8665bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "02625b7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Busted flat in Baton Rouge, waitin' for a train\", \"And I's feelin' near as faded as my jeans\", 'Bobby thumbed a diesel down, just before it rained']\n"
     ]
    }
   ],
   "source": [
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus에 담기\n",
    "\n",
    "for txt_file in txt_list:         \n",
    "    with open(txt_file, \"r\") as f:       # 파일을 읽기모드로 열기.\n",
    "        raw = f.read().splitlines()      # 라인 단위로 끊어서 list 형태로 읽어오기\n",
    "        raw_corpus.extend(raw)           # raw_corpus에 넣기     \n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])    # 3개의 라인 확인하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0cade8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Busted flat in Baton Rouge, waitin' for a train\n",
      "And I's feelin' near as faded as my jeans\n",
      "Bobby thumbed a diesel down, just before it rained\n",
      "It rode us all the way to New Orleans I pulled my harpoon out of my dirty red bandanna\n",
      "I was playin' soft while Bobby sang the blues, yeah\n",
      "Windshield wipers slappin' time, I was holdin' Bobby's hand in mine\n",
      "We sang every song that driver knew Freedom's just another word for nothin' left to lose\n",
      "Nothin', don't mean nothin' hon' if it ain't free, no no\n",
      "And, feelin' good was easy, Lord, when he sang the blues\n",
      "You know, feelin' good was good enough for me\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue       # 길이가 0인 문장은 건너뛰기.\n",
    "    \n",
    "    if idx > 9: break                   # 일단 문장 10개만 확인하기.\n",
    "    \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60f51a",
   "metadata": {},
   "source": [
    "# 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65411a9",
   "metadata": {},
   "source": [
    "`preprocess_sentence()` 함수를 만들어서 데이터를 정제.\n",
    "추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거한다.\n",
    "문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c7cb4f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화(Tokenize)\n",
    "\n",
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지.\n",
    "\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ca9cc3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> busted flat in baton rouge , waitin for a train <end>',\n",
       " '<start> and i s feelin near as faded as my jeans <end>',\n",
       " '<start> bobby thumbed a diesel down , just before it rained <end>',\n",
       " '<start> i was playin soft while bobby sang the blues , yeah <end>',\n",
       " '<start> windshield wipers slappin time , i was holdin bobby s hand in mine <end>',\n",
       " '<start> nothin , don t mean nothin hon if it ain t free , no no <end>',\n",
       " '<start> and , feelin good was easy , lord , when he sang the blues <end>',\n",
       " '<start> you know , feelin good was good enough for me <end>',\n",
       " '<start> there bobby shared the secrets of my soul <end>',\n",
       " '<start> through all kinds of weather , through everything we done <end>']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제 데이터 구축\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 원하지 않는 문장을 건너뛰기.\n",
    "    if len(sentence) == 0: continue    # 길이 0\n",
    "    if len(sentence.split()) >= 12: continue # 15개 이하(start, end 포함)\n",
    "    \n",
    "    # 정제를 하고 담기.\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "corpus[:10]  # 정제결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e39d43c",
   "metadata": {},
   "source": [
    "# 4. 평가 데이터셋 분리\n",
    "***\n",
    "**훈련데이터와 평가 데이터를 분리하기**  \n",
    "\n",
    "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상 으로 설정하세요! 총 데이터의 20% 를 평가 데이터셋으로 사용해 주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45d55b66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 3918 1853 ...    0    0    0]\n",
      " [   2    8    5 ...    0    0    0]\n",
      " [   2  849 6699 ...    0    0    0]\n",
      " ...\n",
      " [   2    5   23 ...    0    0    0]\n",
      " [   2    5   23 ...    0    0    0]\n",
      " [   2    5   23 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f28cef4caf0>\n"
     ]
    }
   ],
   "source": [
    "# tokenize() 함수로 데이터를 Tensor로 변환\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words= 12000, #단어장의 크기:12000 이상\n",
    "            filters=' ',\n",
    "            oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2c1d4af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "[   2 3918 1853   14    1 4369    4 1322   28    9  656    3    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n",
      "[3918 1853   14    1 4369    4 1322   28    9  656    3    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "# 단어사전 구축 인덱스 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx,\":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 10: break\n",
    "        \n",
    "#생성된 텐서를 소스와 타겟으로 분리해 모델 학습\n",
    "\n",
    "src_input = tensor[:, :-1]       #소스 문장을 생성\n",
    "tgt_input = tensor[:, 1:]        #타겟 문장을 생성\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "551375dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 29), (256, 29)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터셋 객체 생성\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input)  // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words +1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2cb6c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 데이터의 20%를 평가 데이터셋으로 만들기\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True,\n",
    "                                                          random_state=12\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11676f58",
   "metadata": {},
   "source": [
    "# 5. 인공지능 만들기\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf410ad",
   "metadata": {},
   "source": [
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 `val_loss` 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)\n",
    "\n",
    "그리고 멋진 모델이 생성한 가사 한 줄을 제출하시길 바랍니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d8bc22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256                 # 워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기\n",
    "hidden_size = 1024                   # 모델에 얼마나 많은 일꾼을 둘 것인지.\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b3e994df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 29, 12001), dtype=float32, numpy=\n",
       "array([[[ 1.98183276e-04,  1.10739304e-04,  1.50641514e-04, ...,\n",
       "         -7.86882374e-05, -3.52473508e-05,  9.43987179e-05],\n",
       "        [ 3.91557514e-05,  1.60865038e-05,  9.94312795e-05, ...,\n",
       "         -1.77722177e-04, -2.37491229e-04,  1.11649570e-04],\n",
       "        [-1.47601910e-04, -2.40673253e-04,  3.54548902e-05, ...,\n",
       "         -3.41957522e-04, -4.39740514e-04, -8.09758640e-05],\n",
       "        ...,\n",
       "        [ 1.48985570e-03, -1.35596230e-04, -8.59143693e-05, ...,\n",
       "         -1.89919746e-03,  2.09501100e-04,  6.66902051e-05],\n",
       "        [ 1.49437541e-03, -7.36683651e-05, -1.48865787e-04, ...,\n",
       "         -1.93941174e-03,  2.51097284e-04,  4.80871931e-05],\n",
       "        [ 1.49734819e-03, -1.77659876e-05, -2.05424600e-04, ...,\n",
       "         -1.97252841e-03,  2.89809686e-04,  3.09579118e-05]],\n",
       "\n",
       "       [[ 1.98183276e-04,  1.10739304e-04,  1.50641514e-04, ...,\n",
       "         -7.86882374e-05, -3.52473508e-05,  9.43987179e-05],\n",
       "        [ 4.02361096e-04,  2.05652934e-04,  1.25637700e-04, ...,\n",
       "         -2.92202720e-04,  3.67133354e-04,  4.18255768e-05],\n",
       "        [ 5.32521342e-04,  3.30442708e-04, -1.66210972e-04, ...,\n",
       "         -3.20876832e-04,  4.50643565e-04,  1.58544761e-04],\n",
       "        ...,\n",
       "        [ 1.29410019e-03,  1.71873631e-04, -3.61978135e-04, ...,\n",
       "         -1.95814040e-03,  4.27003019e-04, -1.95720168e-05],\n",
       "        [ 1.32030528e-03,  1.98288020e-04, -3.87876964e-04, ...,\n",
       "         -1.98856555e-03,  4.44215839e-04, -3.07383780e-05],\n",
       "        [ 1.34342583e-03,  2.21125054e-04, -4.12298134e-04, ...,\n",
       "         -2.01293733e-03,  4.60816314e-04, -4.02198821e-05]],\n",
       "\n",
       "       [[ 1.98183276e-04,  1.10739304e-04,  1.50641514e-04, ...,\n",
       "         -7.86882374e-05, -3.52473508e-05,  9.43987179e-05],\n",
       "        [ 4.27850930e-04,  3.76695010e-04,  3.69281130e-04, ...,\n",
       "          3.82213802e-05,  1.61651697e-05,  1.30311309e-04],\n",
       "        [ 4.86179692e-04,  3.73523479e-04,  3.54190590e-04, ...,\n",
       "          1.18671473e-04, -4.39846335e-05,  2.72889156e-04],\n",
       "        ...,\n",
       "        [ 1.28294062e-03, -1.31786481e-04, -2.19498208e-04, ...,\n",
       "         -1.75800070e-03,  3.63680709e-04, -1.03674911e-05],\n",
       "        [ 1.31851505e-03, -6.65396874e-05, -2.59008375e-04, ...,\n",
       "         -1.82189303e-03,  3.85238265e-04, -2.52844093e-05],\n",
       "        [ 1.34845090e-03, -7.74252931e-06, -2.95446749e-04, ...,\n",
       "         -1.87557586e-03,  4.05786210e-04, -3.80755882e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.98183276e-04,  1.10739304e-04,  1.50641514e-04, ...,\n",
       "         -7.86882374e-05, -3.52473508e-05,  9.43987179e-05],\n",
       "        [ 3.30361829e-04,  1.48775027e-04,  4.66678699e-04, ...,\n",
       "         -3.10520205e-04, -4.49942709e-05,  1.73819208e-04],\n",
       "        [ 1.86484860e-04, -1.42458530e-05,  5.97031671e-04, ...,\n",
       "         -3.07611888e-04, -2.42136914e-04, -1.44765972e-05],\n",
       "        ...,\n",
       "        [ 1.05105434e-03, -1.63013727e-04,  3.26424983e-04, ...,\n",
       "         -1.67183252e-03,  9.96512463e-05,  1.88039470e-04],\n",
       "        [ 1.11294514e-03, -1.05909596e-04,  2.20690315e-04, ...,\n",
       "         -1.72487274e-03,  1.36556322e-04,  1.69455816e-04],\n",
       "        [ 1.16973952e-03, -5.00593815e-05,  1.23046644e-04, ...,\n",
       "         -1.77561061e-03,  1.74110610e-04,  1.51024593e-04]],\n",
       "\n",
       "       [[ 1.98183276e-04,  1.10739304e-04,  1.50641514e-04, ...,\n",
       "         -7.86882374e-05, -3.52473508e-05,  9.43987179e-05],\n",
       "        [ 4.34696412e-04,  1.36360439e-04,  1.94362845e-04, ...,\n",
       "         -4.83703683e-04, -3.03491324e-05,  1.65853446e-04],\n",
       "        [ 7.32702611e-04, -5.52855709e-05,  7.20325843e-05, ...,\n",
       "         -7.33746332e-04,  6.57049168e-05,  8.39911991e-06],\n",
       "        ...,\n",
       "        [ 1.31365831e-03, -1.01726273e-05, -1.92460313e-04, ...,\n",
       "         -1.88218255e-03,  2.99844745e-04, -1.01349069e-04],\n",
       "        [ 1.34323572e-03,  3.39999788e-05, -2.31565646e-04, ...,\n",
       "         -1.92821899e-03,  3.22506152e-04, -1.05260413e-04],\n",
       "        [ 1.36839389e-03,  7.43585260e-05, -2.69132346e-04, ...,\n",
       "         -1.96610950e-03,  3.45393113e-04, -1.07996537e-04]],\n",
       "\n",
       "       [[ 1.98183276e-04,  1.10739304e-04,  1.50641514e-04, ...,\n",
       "         -7.86882374e-05, -3.52473508e-05,  9.43987179e-05],\n",
       "        [ 3.59989819e-04,  1.79269045e-04,  9.63393977e-05, ...,\n",
       "          4.41336051e-05, -1.15099254e-04,  3.17139755e-04],\n",
       "        [ 6.65652915e-04, -1.02989488e-05, -8.40267676e-05, ...,\n",
       "         -3.36554294e-05, -9.31937102e-05,  2.04772514e-04],\n",
       "        ...,\n",
       "        [ 1.30810903e-03,  9.83391074e-05, -3.41035862e-04, ...,\n",
       "         -2.02089781e-03,  3.05706897e-04, -6.59248326e-05],\n",
       "        [ 1.33363693e-03,  1.27824867e-04, -3.74249503e-04, ...,\n",
       "         -2.04002461e-03,  3.42146144e-04, -6.99578959e-05],\n",
       "        [ 1.35590474e-03,  1.54240508e-04, -4.03796643e-04, ...,\n",
       "         -2.05478794e-03,  3.74933472e-04, -7.41944168e-05]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오기\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "model(src_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a0ac4cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1b43d87a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "476/476 [==============================] - 188s 390ms/step - loss: 1.7567 - val_loss: 1.5098\n",
      "Epoch 2/10\n",
      "476/476 [==============================] - 198s 416ms/step - loss: 1.4518 - val_loss: 1.4186\n",
      "Epoch 3/10\n",
      "476/476 [==============================] - 199s 417ms/step - loss: 1.4031 - val_loss: 1.3811\n",
      "Epoch 4/10\n",
      "476/476 [==============================] - 199s 419ms/step - loss: 1.3281 - val_loss: 1.3381\n",
      "Epoch 5/10\n",
      "476/476 [==============================] - 200s 419ms/step - loss: 1.2779 - val_loss: 1.3077\n",
      "Epoch 6/10\n",
      "476/476 [==============================] - 200s 420ms/step - loss: 1.2339 - val_loss: 1.2823\n",
      "Epoch 7/10\n",
      "476/476 [==============================] - 200s 420ms/step - loss: 1.1929 - val_loss: 1.2617\n",
      "Epoch 8/10\n",
      "476/476 [==============================] - 200s 419ms/step - loss: 1.1548 - val_loss: 1.2433\n",
      "Epoch 9/10\n",
      "476/476 [==============================] - 199s 419ms/step - loss: 1.1191 - val_loss: 1.2279\n",
      "Epoch 10/10\n",
      "476/476 [==============================] - 200s 420ms/step - loss: 1.0857 - val_loss: 1.2152\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습하기\n",
    "history= []\n",
    "epochs = 10\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "history = model.fit(enc_train,\n",
    "                    dec_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(enc_val,dec_val)\n",
    "                    ,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7711dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    while True:\n",
    "        # 입력받은 문장의 텐서를 입력\n",
    "        predict = model(test_tensor) \n",
    "        # 예측된 값 중 가장 높은 확률인 word index를 뽑기\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 예측된 word index를 문장 뒤에 붙임\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마침\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4a071dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , liberian girl <end> '"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#문장 생성 함수 실행\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d186862",
   "metadata": {},
   "source": [
    "# 회고\n",
    "***\n",
    "이번 EXPLORATION 6번을 하면서 내용이 이해가 안되는 것이 많았다.  \n",
    "시퀀스 데이터부터 데이터를 다듬고 학습시키고 실제로 문장을 생성하는 것 까지 생소했던 것 같다.  \n",
    "전처리하는 과정에서 토큰화, 텐서를 하는 과정이 중요하고 시간도 걸리는 것 같다.  \n",
    "데이터 정제하는 과정에 따라 모델이 학습하고 성능이 달라지는 것을 확인하였다.  \n",
    "토큰의 개수를 정하는 이유와 tokenize 함수에 대한 부분도 좀 더 이해가 필요한 것 같다.  \n",
    "실제로 학습시키고 val_loss 값이 1.2152로 비교적 잘 학습이 된 것같다.  \n",
    "문장을 생성하는 함수를 실행했을때  \n",
    "'liberian girl'이란 단어가 나왔는데 마이클 잭슨의 노래에 있는 가사라는 것을 확인하였다.  \n",
    "모델이 문장 생성을 잘 했다고 생각하지만, 쉽지 않은 실습이였던 것 같다.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
